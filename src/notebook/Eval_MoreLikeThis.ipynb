{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook have almost all the backend code needed for you to run an interface to evaluate the documents more like this is returning are relevant to your solution.\n",
    "\n",
    "The Azure Services we are using in this case:\n",
    "* Azure Search\n",
    "* Azure Storage Account\n",
    "    * Containers\n",
    "    * Tables\n",
    "\n",
    "The code below can be implemented directly in a WebApp or in Azure Functions, it's up to you!\n",
    "\n",
    "Basic functionality:\n",
    "\n",
    "This function will run a search in Azure Search, and retrieve 20% of all the documents that we will be used as baseline to test More Like This API.\n",
    "\n",
    "In the code below I explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing: the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will create the string that will allow us to connect to the Azure Search service and define the values we will be using through the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables defined in the following section are self-explainable, but some worth mention:\n",
    "\n",
    "* searchSvc: the search service you created in Azure;\n",
    "* headers { api-key }: the key for the search service;\n",
    "* searchFields: the field you want to use in More Like This when it searchs the other documents;\n",
    "* totalDocuments: the total number of documents in your Storage Account;\n",
    "* userName: used as PartitionKey in Azure Table.\n",
    "* rowKey: the RowKey in Azure Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchSvc = os.environ['SEARCH_SVC']\n",
    "endpoint = 'https://'+ searchSvc + '.search.windows.net/'\n",
    "apiVersion = '?api-version=2019-05-06-Preview'\n",
    "headers = {'Content-Type': 'application/json',\n",
    "           'api-key': os.environ['SEARCH_SUBSCRIPTION_KEY']}\n",
    "searchFields = 'merged_content'\n",
    "numMoreLikeThisDocs = '3'\n",
    "\n",
    "totalDocuments = 50\n",
    "numDocsToSelect = int(totalDocuments*0.2)\n",
    "userName = 'Squassina'\n",
    "\n",
    "rowKey = str(datetime.date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will retrieve the possible indexes. By default I am considering there is only one index in the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = endpoint + \"indexes\" + apiVersion + \"&$select=name\"\n",
    "response  = requests.get(url, headers=headers)\n",
    "indexList = response.json()\n",
    "indexName = indexList['value'][0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function if to select the documents I'm using to evaluate the service. \n",
    "\n",
    "I will select a random integer (a) to elect the document I will use to test if More Like This is returning relevant documents.\n",
    "\n",
    "By using Azure Search to retrieve the first document after skipping (a), I'm selecting only a few fields to accelerate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_source_documents():\n",
    "    random.seed()\n",
    "    randDoc = random.randint(0,totalDocuments)\n",
    "    url = endpoint + 'indexes/' + indexName + '/docs' + apiVersion + '&$select=Id,metadata_storage_path&$top=1&$skip=' + str(randDoc)\n",
    "    response  = requests.get(url, headers=headers)\n",
    "    return(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will retrieve the documents returned by More Like This for me to do the evaluation. Here, two key fields worth noticing:\n",
    "* moreLikeThis; and\n",
    "* searchField\n",
    "\n",
    "According to the [documentation](https://docs.microsoft.com/en-us/azure/search/search-more-like-this): \n",
    "> ` moreLikeThis=[key] ` is a query parameter in the Search Documents API that finds documents similar to the document specified by the document key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_like_this_docs(odocId: str):\n",
    "    url = endpoint + 'indexes/' + indexName + '/docs' + apiVersion + '&$moreLikeThis='+ odocId + '&searchFields=' + searchFields + '&$top=' + numMoreLikeThisDocs\n",
    "    response  = requests.get(url, headers=headers)\n",
    "    return(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following 2 sections we start the process to select the documents and the results of More Like This to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDocList = {}\n",
    "\n",
    "for i in range(numDocsToSelect):\n",
    "    sourceDocList[i] = (select_source_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt = {}\n",
    "\n",
    "for i in range(len(sourceDocList)):\n",
    "    mlt[i] = more_like_this_docs(sourceDocList[i]['value'][0]['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I send everything to a Pandas DataFrame before storing for better visualization, because I am working with a very small set of documents, but you may skip this section and move to Store data in Azure Storage Account / Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDf = pd.DataFrame(columns = ['RowKey','PartitionKey',\n",
    "                                  'source_id','source_stg_path',\n",
    "                                  'MLT_1_id','MLT_1_stg_path','MLT_1_score',\n",
    "                                  'MLT_2_id','MLT_2_stg_path','MLT_2_score',\n",
    "                                  'MLT_3_id','MLT_3_stg_path','MLT_3_score',\n",
    "                                  'EVAL_1', 'EVAL_2', 'EVAL_3','DCG'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice well, there are 4 additional fields I created above that I'm not using below: `EVAL_1, EVAL_2, EVAL_3, DCG`. These fields will be used when you are doing the evaluation in the UI: one evaluation for each document returned by MoreLikeThis and the final calculation of the relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDf.RowKey = [rowKey + ' - ' + str(x) for x in range(len(sourceDocList))]\n",
    "docsDf.PartitionKey = [userName for x in range(len(sourceDocList))]\n",
    "docsDf.source_id = [sourceDocList[i]['value'][0]['Id'] for i in range(len(sourceDocList))]\n",
    "docsDf.source_stg_path = [sourceDocList[i]['value'][0]['metadata_storage_path'] for i in range(len(sourceDocList))]\n",
    "docsDf.MLT_1_id = [mlt[i]['value'][0]['Id'] for i in range(len(mlt))]\n",
    "docsDf.MLT_1_stg_path = [mlt[i]['value'][0]['metadata_storage_path'] for i in range(len(mlt))]\n",
    "docsDf.MLT_1_score = [mlt[i]['value'][0]['@search.score'] for i in range(len(mlt))]\n",
    "docsDf.MLT_2_id = [mlt[i]['value'][1]['Id'] for i in range(len(mlt))]\n",
    "docsDf.MLT_2_stg_path = [mlt[i]['value'][1]['metadata_storage_path'] for i in range(len(mlt))]\n",
    "docsDf.MLT_2_score = [mlt[i]['value'][1]['@search.score'] for i in range(len(mlt))]\n",
    "docsDf.MLT_3_id = [mlt[i]['value'][2]['Id'] for i in range(len(mlt))]\n",
    "docsDf.MLT_3_stg_path = [mlt[i]['value'][2]['metadata_storage_path'] for i in range(len(mlt))]\n",
    "docsDf.MLT_3_score = [mlt[i]['value'][2]['@search.score'] for i in range(len(mlt))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDf.to_csv('./docs_to_be_evaluated.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store data in Azure Storage Account / Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't want to work in memory and loose everything I have done so far, so I store the results of above code in Azure Tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imports for this section of the code are for Azure Storage. If you need to install it, the command is: \n",
    "\n",
    "`pip install azure-storage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage import CloudStorageAccount\n",
    "from azure.storage.table import TableService, Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDict = pd.read_csv('./docs_to_be_evaluated.csv').to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm setting the account by reading the Storage Account Name and Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountName = os.environ['STORAGE_ACCOUNT_NAME']\n",
    "\n",
    "accountKey = os.environ['STORAGE_ACCOUNT_KEY']\n",
    "\n",
    "account = CloudStorageAccount(accountName, accountKey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the table service. I am using a table called `MLTEval` to store the data from the first part of this solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableName = 'MLTEval'\n",
    "tableService = account.create_table_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For educational purposes only, I'm leaving the code to delete and create table here, but it's not needed after the first run as we will save this to compare after the users start using the system that the documents retrieved are still relevant, or it's needed to adjust something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_service.delete_table(tableName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_table` wil return `true` if the table was created or `false`, if the table already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableService.create_table(tableName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm inserting the data I collected in the first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(docsDict)):\n",
    "    tableService.insert_or_replace_entity(tableName, docsDict[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Discount Cumulative Gain (DCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will calculate DCG. I didn't wrote it, the code is available in the website as indicated in the comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=0):\n",
    "    import numpy as np\n",
    "    # https://gist.github.com/bwhite/3726239\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will take partitionKey and rowKey as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data_azure_table(partitionKey : str, rowKey : str):\n",
    "    import os\n",
    "    from azure.storage import CloudStorageAccount\n",
    "    from azure.storage.table import TableService, Entity\n",
    "    import numpy as np\n",
    "\n",
    "    accountName = os.environ['STORAGE_ACCOUNT_NAME']\n",
    "    \n",
    "    accountKey = os.environ['STORAGE_ACCOUNT_KEY']\n",
    "    \n",
    "    account = CloudStorageAccount(accountName, accountKey)\n",
    "\n",
    "    filter = \"PartitionKey eq '\" + partitionKey + \"' and RowKey gt '\" + rowKey + \"'\"\n",
    "\n",
    "    tableService = account.create_table_service()\n",
    "    \n",
    "    return(tableService.query_entities(table_name = tableName, filter = filter))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using the userName as partitionKey in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDf = retrieve_data_azure_table(partitionKey = userName, rowKey = rowKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running DCG in the data as is will return NaN as we don't have any evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "DCG = []\n",
    "for row in docsDf:            \n",
    "    dcg = dcg_at_k( [row['EVAL_1'], row['EVAL_2'], row['EVAL_3']], k, 1 )\n",
    "    DCG.append(dcg)\n",
    "    \n",
    "dcgDf = pd.DataFrame( data = {'DCG':DCG} )\n",
    "print(dcgDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some data to EVAL_* fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in docsDf:            \n",
    "    row['EVAL_1'] = random.randint(0,4)\n",
    "    row['EVAL_2'] = random.randint(0,4)\n",
    "    row['EVAL_3'] = random.randint(0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have data to calculate DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "DCG = []\n",
    "for row in docsDf:            \n",
    "    dcg = dcg_at_k( [row['EVAL_1'], row['EVAL_2'], row['EVAL_3']], k, 1 )\n",
    "    row['DCG'] = str(dcg)\n",
    "    DCG.append(dcg)\n",
    "    \n",
    "dcgDf = pd.DataFrame( data = {'DCG':DCG} )\n",
    "print(dcgDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save everything to Azure Tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in docsDf.items:\n",
    "    tableService.insert_or_replace_entity(tableName, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
